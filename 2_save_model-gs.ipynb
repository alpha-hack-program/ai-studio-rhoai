{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model and the Evaluation Data\n",
    "\n",
    "To save this model and the evaluation data so that you can use it from various locations, including other notebooks or the model server, upload it to s3-compatible storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install the required packages and define a function for the upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up the GCS credentials and bucket name\n",
    "gcs_credentials_path = os.environ.get('GS_SERVICE_ACCOUNT_KEY')\n",
    "bucket_name = os.environ.get('GS_BUCKET_NAME')\n",
    "\n",
    "if not all([gcs_credentials_path, bucket_name]):\n",
    "    raise ValueError(\"One or more data connection variables are empty. \"\n",
    "                     \"Please check your data connection to a GCS bucket.\")\n",
    "\n",
    "# Initialize the GCS client\n",
    "storage_client = storage.Client.from_service_account_json(gcs_credentials_path)\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "def upload_file_to_gs(local_file, gcs_blob_name):\n",
    "    blob = bucket.blob(gcs_blob_name)\n",
    "    blob.upload_from_filename(local_file)\n",
    "    print(f\"{local_file} -> {gcs_blob_name}\")\n",
    "\n",
    "def upload_directory_to_gs(local_directory, gcs_prefix):\n",
    "    num_files = 0\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            relative_path = os.path.relpath(file_path, local_directory)\n",
    "            gcs_blob_name = os.path.join(gcs_prefix, relative_path)\n",
    "            upload_file_to_gcs(file_path, gcs_blob_name)\n",
    "            num_files += 1\n",
    "    return num_files\n",
    "\n",
    "def list_objects(prefix):\n",
    "    blobs = storage_client.list_blobs(bucket_name, prefix=prefix)\n",
    "    for blob in blobs:\n",
    "        print(blob.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Verify the upload\n",
    "\n",
    "In your S3 bucket, under the `models` upload prefix, run the `list_object` command. As best practice, to avoid mixing up model files, keep only one model and its required files in a given prefix or directory. This practice allows you to download and serve a directory with all the files that a model requires. \n",
    "\n",
    "If this is the first time running the code, this cell will have no output.\n",
    "\n",
    "If you've already uploaded your model, you should see this output: `models/fraud/1/model.onnx`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/\n"
     ]
    }
   ],
   "source": [
    "list_objects(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've already uploaded your model, you should see this output: `scaler.pkl` and `test_data.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_objects(\"artifact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload and check again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files compressed successfully. Compressed file: models/evaluation_kit.zip\n",
      "models/evaluation_kit.zip -> models/evaluation_kit.zip\n"
     ]
    }
   ],
   "source": [
    "# Compress files: models/fraud/1/model.onnx, artifact/scaler.pkl and artifact/scaler.pkl using python's zipfile module\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the files to be compressed\n",
    "files_to_compress = [\n",
    "    \"models/fraud/1/model.onnx\",\n",
    "    \"artifact/scaler.pkl\",\n",
    "    \"artifact/test_data.pkl\"\n",
    "]\n",
    "\n",
    "# Define the name of the compressed file\n",
    "compressed_file_name = \"models/evaluation_kit.zip\"\n",
    "\n",
    "# Create a zip file and add the files to it\n",
    "with zipfile.ZipFile(compressed_file_name, 'w') as zipf:\n",
    "    for file in files_to_compress:\n",
    "        zipf.write(file)\n",
    "\n",
    "# Verify the compressed file\n",
    "if os.path.exists(compressed_file_name):\n",
    "    print(f\"Files compressed successfully. Compressed file: {compressed_file_name}\")\n",
    "    # Upload the compressed file to GS\n",
    "    upload_file_to_gs(compressed_file_name, compressed_file_name)\n",
    "else:\n",
    "    print(\"Failed to compress files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function to upload the `models` and `artifact` folders in a rescursive fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/evaluation_kit.zip -> models/evaluation_kit.zip\n",
      "models/fraud/1/model.onnx -> models/fraud/1/model.onnx\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'upload_directory_to_s3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(local_artifacts_directory):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe directory \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_artifacts_directory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid you finish training the model in the previous notebook?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m num_files \u001b[38;5;241m=\u001b[39m \u001b[43mupload_directory_to_s3\u001b[49m(local_artifacts_directory, local_artifacts_directory)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_files \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo files uploaded.  Did you finish training and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaving the model to the \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124martifacts\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m directory?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'upload_directory_to_s3' is not defined"
     ]
    }
   ],
   "source": [
    "local_models_directory = \"models\"\n",
    "\n",
    "if not os.path.isdir(local_models_directory):\n",
    "    raise ValueError(f\"The directory '{local_models_directory}' does not exist.  \"\n",
    "                     \"Did you finish training the model in the previous notebook?\")\n",
    "\n",
    "num_files = upload_directory_to_gs(\"models\", \"models\")\n",
    "\n",
    "if num_files == 0:\n",
    "    raise ValueError(\"No files uploaded.  Did you finish training and \"\n",
    "                     \"saving the model to the \\\"models\\\" directory?  \"\n",
    "                     \"Check for \\\"models/fraud/1/model.onnx\\\"\")\n",
    "\n",
    "local_artifacts_directory = \"artifact\"\n",
    "\n",
    "if not os.path.isdir(local_artifacts_directory):\n",
    "    raise ValueError(f\"The directory '{local_artifacts_directory}' does not exist.  \"\n",
    "                     \"Did you finish training the model in the previous notebook?\")\n",
    "\n",
    "num_files = upload_directory_to_gs(local_artifacts_directory, local_artifacts_directory)\n",
    "\n",
    "if num_files == 0:\n",
    "    raise ValueError(\"No files uploaded.  Did you finish training and \"\n",
    "                     \"saving the model to the \\\"artifacts\\\" directory?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To confirm this worked, run the `list_objects` function again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_objects(\"models\")\n",
    "list_objects(\"artifact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Step\n",
    "\n",
    "Now that you've saved the model to s3 storage, you can refer to the model by using the same data connection to serve the model as an API.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
